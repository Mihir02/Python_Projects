{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV\n",
    "Coarsely speaking opencv is a **computer vision** library available in *Python*, *C++*, and *Java*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Images and Videos\n",
    "## cv2.imread()\n",
    "It takes the path to an image and returns a matrix of pixel intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"galaxy.jpg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Img\",img)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`cv2.waitKey(int)`** is a keyboard binding function. It waits for a specific delay for a key to be pressed. And in a very counter intuitive manner $0$ implies to wait infinitely for the key press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"OpenCV\\Vid.mp4\")\n",
    "type(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function takes an `int` as an input or a path to the video. The integer value acts as an index for the cameras connected to the system.\n",
    "=> $0$ = webcam\n",
    "\n",
    "Now since a video is just a bunch of still images we can read a video by going through a loop till we've exhausted the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    check, frame = video.read()\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    if cv.waitKey(20) and 0xFF == ord('d'):\n",
    "        break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was an unexpected error. \n",
    "\n",
    "Whenever you get an error like this **`-215 Assertion failed`**, most of the time it implies that opencv wasn't able to find a media file at the specified location. **This is coz our video ran out of frames**.\n",
    "\n",
    "We get the exact same error if we try to read an image but pass the wrong path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('Vid.mp4')\n",
    "\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    print(check)\n",
    "\n",
    "    key = cv2.waitKey(10)     # One frame stays for 10 milisecs\n",
    "    if (not check) or key == ord('q'):    # If there are no frames left\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing and Rescaling\n",
    "*Rescaling* implies modifying height and width to a particular value.\n",
    "Generally we'll prefer to downscale the media files as most cameras cannot go higher than its max value.\n",
    "\n",
    "It works basically anything for which we can use the rescale/resize function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"galaxy.jpg\")\n",
    "\n",
    "def rescale(frame, scale = 0.50):\n",
    "    width = int(frame.shape[1] * scale)\n",
    "    height = int(frame.shape[0] * scale)\n",
    "    dims = (width, height)\n",
    "    return cv2.resize(frame, dims, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('Vid.mp4')\n",
    "\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    frame_res = rescale(frame, scale=2)\n",
    "    #print(check)\n",
    "\n",
    "    key = cv2.waitKey(10)     # One frame stays for 10 milisecs\n",
    "    if (not check) or key == ord('q'):    # If there are no frames left\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame_res)\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of resizing or rescaling video files specifically, using the **`VideoCapture.set`** function. This is specifically for videos and will work on images. **But it works only for live videos**, ie, video that is going on currently and not video files that already exists.\n",
    "\n",
    "The $3$ and $4$ basically stand for the properties of the capture class. \n",
    "- 3 = width\n",
    "- 4 = height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will work only for live videos\n",
    "def changeRes(w, h): \n",
    "    cap.set(3, w)\n",
    "    cap.set(4,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Shapes and Texts on a Video\n",
    "There are two ways of drawing on an image.\n",
    "- Actually drawing on the image \n",
    "- Drawing on a blank image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing on a blank image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('galaxy.jpg',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = np.zeros(img.shape, dtype= 'uint8')\n",
    "\n",
    "blank.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paint the image a color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank[:] = 0, 0, 255\n",
    "cv2.imshow(\"Blue\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Painting a section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank[200:300, 200:300] = 255, 0, 0\n",
    "cv2.imshow(\"Blue\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally ... drawing\n",
    "#### Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(blank, (0,0), (400, 400), (255, 0, 0), 3)\n",
    "cv2.imshow(\"Blue\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of giving a thickness we can pass `cv2.FILLED` to fill the whole region \n",
    "\n",
    "or we can pass -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(blank, (0,0), (400, 400), (255, 0, 0), thickness = cv2.FILLED)\n",
    "cv2.imshow(\"Blue\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(blank, (0,0), (400, 400), (0, 255, 0), thickness = -1)\n",
    "cv2.imshow(\"Rectangle\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of hardcoding we can also pass the dimensions of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(blank, (0,0), (blank.shape[1]//3, blank.shape[0]//2), (0, 255, 0), thickness = -1)\n",
    "cv2.imshow(\"Filled Rectangle\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Circle\n",
    "**Parameters:**\n",
    "- Image\n",
    "- Coordinates of the center\n",
    "- Radius\n",
    "- Color (BGR)\n",
    "- Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.circle(blank, (blank.shape[1]//2, blank.shape[0]//2), 40, (0, 0, 255), thickness= 3)\n",
    "cv2.imshow(\"circle\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.line(blank, (0,0), (blank.shape[1]//2, blank.shape[0]//2), (255, 255, 255), 3)\n",
    "cv2.imshow(\"Line\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing text on an image\n",
    "**Parameters:**\n",
    "- The image\n",
    "- The text\n",
    "- The origin : From where the text should start\n",
    "- Font faces: opencv comes with few in-built fonts\n",
    "- Font scale \n",
    "- Color\n",
    "- Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.putText(blank, \"I have no idea what to type\", (255, 300), cv2.FONT_HERSHEY_PLAIN, 2.5, (0, 255, 0), thickness = 2)\n",
    "cv2.imshow(\"Text\", blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions of OpenCV\n",
    "## Converting an image to grayscale\n",
    "We can import images directly as a grayscale while reading in `imread` by passing an int along with the path\n",
    "- $0$ => Grayscale\n",
    "- $1$ => BGR\n",
    "- $-1$ => BGR with some bells and whistles (an alpha channel)\n",
    "\n",
    "Grayscale basically means we focus on intensity distribution of the pixels rather than the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"galaxy.jpg\",0)\n",
    "cv2.imshow(\"Name\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can convert a color image to grayscales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"galaxy.jpg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"Name\", gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very common thing\n",
    "## Bluring\n",
    "**Smoothing**, also called *blurring*, is a simple and frequently used image processing operation.\n",
    "\n",
    "Bluring an image removes some of the noise that is in the image (which can be because of extra elements due to bad lighting or some issue with the sensor or ...)\n",
    "\n",
    "There are many, many bluring techinques but we'll focus on **Gaussian Blur**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a smoothing operation we will apply a filter to our image. The most common type of filters are linear, in which an output pixel's value (i.e. $g(i,j)$ ) is determined as a weighted sum of input pixel values (i.e. $f(i+k,j+l)$ ) :\n",
    "\n",
    "$g(i,j) = \\sum_{k,l}^{} f(i+k,j+l)\\bf{h}(k,l)$\n",
    "\n",
    "Here the $\\bf{h}(k,l)$ is called the *kernel*, which is the coefficients of the filter.\n",
    "\n",
    "We can think of a filter as a window of coefficients sliding across the image.\n",
    "\n",
    "**The Gaussian Filter**\n",
    "\n",
    "Gaussian filtering is done by convolving (weighing) each point in the input array with a Gaussian kernel and then summing them all to produce the output array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more on smoothing go to -https://docs.opencv.org/4.5.3/dc/dd3/tutorial_gausian_median_blur_bilateral_filter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use image `sky.jpg`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"galaxy.jpg\")\n",
    "image = rescale(image, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "- `src` or Source image\n",
    "- `ksize` or Kernel size: $2X2$ tuple which is the window size that *opencv* uses to compute the blur on the image (**must be an odd number**)\n",
    "- `sigmaX` : Standard deviation of the kernel along the horizontal direction.\n",
    "- `sigmaY` : Same but along the vertical direction.\n",
    "- `borderType` : Specifies the boundaries of an image while kernel is applied on the borders of an image\n",
    "        - cv2.BORDER_CONSTANT\n",
    "        - cv2.BORDER_REPLICATE\n",
    "        - cv2.BORDER_REFLECT\n",
    "        - cv2.BORDER_WRAP\n",
    "        - cv2.BORDER_REFLECT_101\n",
    "        - cv2.BORDER_TRANSPARENT\n",
    "        - cv2.BORDER_REFLECT101\n",
    "        - cv2.BORDER_DEFAULT\n",
    "        - cv2.BORDER_ISOLATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blured = cv2.GaussianBlur(image, (3,3), cv2.BORDER_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Blured\", blured)\n",
    "cv2.imshow(\"OG Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Cascade\n",
    "Used to find edges present in an image. There are many edge cascades that are available but we'll be using the *canny edge detector*. Just to give an idea of how old these techinques are, this detector was developed by *John F. Canny* in $1986$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge Detector\n",
    "It is a multi-step process that invloves a lot of blurring and then lots of grading computations.\n",
    "\n",
    "The steps in the algorithm are:\n",
    "- Preprocessing\n",
    "- Calculating Gradients\n",
    "- Nonmaximum Suppression\n",
    "- Thresholding with hysterysis\n",
    "\n",
    "The two key parameters of the algorithm are - *an upper threshold* and *a lower threshold*. The upper threshold is used to mark edges that are definitely edges. The lower threshold is to find faint pixels that are actually a part of an edge.\n",
    "\n",
    "For more details visit: https://aishack.in/tutorials/canny-edge-detector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(image, 125, 175)\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.imshow(\"Edges\", canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTICE** that are hardly any edges detected in the sky. Thats why *preprocessing/smoothing* is important. And the details in the sky are example of unimportant details. We can further reduce many unneccessary edges by passing the *blured* image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(blured, 125, 175)\n",
    "cv2.imshow(\"Image\", blured)\n",
    "cv2.imshow(\"Edges\", canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, kinda makes more sense. What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilating an image\n",
    "using specific structuring elements.\n",
    "\n",
    "The structuring elements that we're going to use are the edges we found. \n",
    "\n",
    "Lets test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilate = cv2.dilate(canny, (15,15), iterations = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "- Structuring element, `src`\n",
    "- `ksize`\n",
    "- `iterations` : Dilation can be applied using several iterations at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Dilate\", dilate)\n",
    "cv2.imshow(\"Structuring Elements\", canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that depending on the kernel size and the number of iterations the dilated edges have their new thickness.\n",
    "\n",
    "Now we can get back the orignal structuring element by *eroding* the dilated image. It won't be perfect but will work in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eroded = cv2.erode(dilate, (15,15), iterations = 3)\n",
    "cv2.imshow(\"Dilate\", dilate)\n",
    "cv2.imshow(\"Structuring Elements\", canny)\n",
    "cv2.imshow(\"Erode\", eroded)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the kernel size and iterations equal will result in an eroded cascade which will be quite similar to the orignal edge cascade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing and cropping an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(image, (500,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Image\",image)\n",
    "cv2.imshow(\"Resized\", resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In backgrounf there is an interpolation that occurs, and we can control it.\n",
    "\n",
    "By default it is set to `interpolation = cv2.INTER_AREA`, which is quite good for shrinking images, but depending on out task we can change the interpolation algorithm. Others that we have are\n",
    "- `cv2.INTER_LINEAR` - Good for zooming\n",
    "- `cv2.INTER_CUBIC` - Very good for zooming and purposes but is the slowest of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(image, (500,500), interpolation = cv2.INTER_CUBIC) # We also have INTER_LINEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cropping can be easily done by using **array slicing** since images are basically arrays of pixel intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Transformations \n",
    "Some common transformation techniques are:\n",
    "- Translation\n",
    "- Rotation\n",
    "- Resizing\n",
    "- Flipping\n",
    "- Clipping\n",
    "- Cropping\n",
    "## Translation\n",
    "Like with translational motion translation of an image is shifting an image along the x and y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(img, x, y):   #x & y = number of pixels by which it is to be shifted\n",
    "    tMat = np.float32([[1,0,x],[0,1,y]])\n",
    "    dims = (img.shape[1], img.shape[0])\n",
    "    return cv2.warpAffine(img, tMat, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = trans(image, -100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"IM\", image)\n",
    "cv2.imshow(\"Shifted\", tran)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation\n",
    "We  can specify any point of the image to rotate the image around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot(img, ang, rpt=None):\n",
    "    (h, w) = img.shape[:2]\n",
    "\n",
    "    if rpt is None:\n",
    "        rpt = (w//2, h//2)\n",
    "    \n",
    "    rMat = cv2.getRotationMatrix2D(rpt, ang, 1.0)\n",
    "    dims = (w,h)\n",
    "\n",
    "    return cv2.warpAffine(img, rMat, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated = rot(image, 45)\n",
    "rotated1 = rot(rotated, 45)\n",
    "rotated2 = rot(rotated1, 45)\n",
    "cv2.imshow(\"IMG\", image)\n",
    "cv2.imshow(\"Change\", rotated)\n",
    "cv2.imshow(\"Change1\", rotated1)\n",
    "cv2.imshow(\"Change2\", rotated2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on each rotation some black regions are introduced in the image and on next rotation those black regions are also rotated (coz now they are part of the image)\n",
    "## Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(image, (1000,1000), interpolation = cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"IMG\", image)\n",
    "cv2.imshow(\"Change\", resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping\n",
    "**Parameters:**\n",
    "- Sourse\n",
    "- Flip code : 0, 1 or -1\n",
    "\n",
    "0 => vertically\n",
    "\n",
    "1 => horizontally\n",
    "\n",
    "-1 => both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = cv2.flip(image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"IMG\", image)\n",
    "cv2.imshow(\"Change\", flipped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping\n",
    "Basically array slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = image[200:400, 100:300]\n",
    "cv2.imshow(\"IMG\", image)\n",
    "cv2.imshow(\"Change\", cropped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour Detection\n",
    "Contours are, simply put, boundaries of objects. They are the curve that joins the continuous points along the boundary of objects. Mathematically they aren't the same as edges, though we can use the term interchangably in *some cases*. \n",
    "\n",
    "Contours are the boundaries of a shape with same intensity and are very useful for shape analysis, object detection and recognition.\n",
    "\n",
    "In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"galaxy.jpg\")\n",
    "img = rescale(img)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Now we want the edges of the image \n",
    "\n",
    "cann = cv2.Canny(img, 125, 175)\n",
    "\n",
    "# To find contours we use\n",
    "contours_cann, hierarchies = cv2.findContours(cann, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'{len(contours)} contours found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of contours. Now lets try it out with blurred image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"galaxy.jpg\")\n",
    "img = rescale(img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "blur = cv2.GaussianBlur(gray, (3,3), cv2.BORDER_DEFAULT)\n",
    "# Now we want the edges of the image \n",
    "\n",
    "cannb = cv2.Canny(blur, 125, 175)\n",
    "\n",
    "# To find contours we use\n",
    "contours_cannb, hierarchies = cv2.findContours(cannb, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'{len(contours)} contours found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a significant decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output are contours (list of all the contours) and hierarchies, which are hierarchical representation of contours. For example if there are different shapes one inside another then these *hierarchies* is the representation that opencv uses to find those contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the contours we use the `cv2.findContours` function which takes the following parameters:\n",
    "- `image`\n",
    "- `mode` in which to find and return the contours. This is either `cv2.RETR_TREES` for all the hierarchical contours or `cv2.RETR_EXTERNAL` if we want only the external contours or `cv2.RETR_LIST` for all the contours in the image\n",
    "- `method` is the *contour approximation method*.\n",
    "\n",
    "Contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But does it store all the coordinates ? That is specified by this *contour approximation method*.\n",
    "- `cv.CHAIN_APPROX_NONE` : all the boundary points are stored.\n",
    "\n",
    "But we don't need all the points to represent a curve or a line. For eg, we need only 2 points to represent a line, thus we have\n",
    "- `cv.CHAIN_APPROX_SIMPLE` : It removes all redundant points and compresses the contour, thereby saving memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"IMG\", img)\n",
    "cv2.imshow(\"cann\", cann)\n",
    "cv2.imshow(\"cannb\", cannb)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This made is quite clear that the blurred image got rid of a huge amount of unnecessary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of finding contours is using **threshold** instead of using the *Canny edge detector*. \n",
    "\n",
    "Thresholding is kinda like *binarizing* an image, so if a particualr pixel has intensity below the threshold it will be set to $0$ and if it above threshold it will be set to what we specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, thresh = cv2.threshold(gray, 125, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "- image\n",
    "- `thresh` : thereshold value for pixel intensity\n",
    "- `type` of thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_th, hierarchies = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'{len(contours)} contours found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite a decrease (nearly half) when we used the *thresholded image* to find the contours. Lets visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"IMG\", img)\n",
    "cv2.imshow(\"cann\", cann)\n",
    "cv2.imshow(\"cannb\", cannb)\n",
    "cv2.imshow(\"Thresh\", thresh)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the contours found in the image by simply drawing over the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "blank_cann = np.zeros(img.shape, dtype = 'uint8')\n",
    "blank_cannb = np.zeros(img.shape, dtype = 'uint8')\n",
    "blank_thres = np.zeros(img.shape, dtype = 'uint8')\n",
    " # Now we draw the contours on the blank image\n",
    "\n",
    "cv2.drawContours(blank_thres, contours_th, -1, (0,0,255), 1)\n",
    "cv2.drawContours(blank_cann, contours_cann, -1, (0,0,255), 1)\n",
    "cv2.drawContours(blank_cannb, contours_cannb, -1, (0,0,255), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `cv2.drawContours` function takes:\n",
    "- an **image** to draw over\n",
    "- a **list** of contours\n",
    "- a contour index : basically, number of contours we want in the image. $-1$ => All of the contours.\n",
    "- color tuple\n",
    "- thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow(\"IMG\", img)\n",
    "#cv2.imshow(\"cann\", cann)\n",
    "#cv2.imshow(\"cannb\", cannb)\n",
    "cv2.imshow(\"contours_thres\", contours_th)\n",
    "cv2.imshow(\"contours_cann\",contours_cann)\n",
    "cv2.imshow(\"contours_cannb\",contours_cannb)\n",
    "cv2.imshow(\"Thresh\", thresh)\n",
    "cv2.imshow(\"\",)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those were the contours obtained from the thresholded image.\n",
    "\n",
    "Though simple to do, this type of thresholding has its disadvantages so its better to go with the *Canny edge detection* and then contour finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9da5a1eb768521143534bf09b996f0d525d45d2d4b488be6c4304e80c1815fea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
