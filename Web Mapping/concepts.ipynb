{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PANDAS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas is a software library written for the Python programming language for **data manipulation and analysis**. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrames "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data structures that store data in pandas are the **Data Frames**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.DataFrame([[2, 4, 6],  [10, 20, 30]])\r\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 0, 1, 2 at top are the **column names** (you can name the columns yourself) and the 0 and 1 in the right are the indices (which also can be renamed)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.DataFrame([[2, 4, 6],  [10, 20, 30]], columns = [\"Column1\", \"Columns2\", \"Column3\"], index= [\"Row1\", \"Row2\"])\r\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "See..... the names\r\n",
    "\r\n",
    "Though the customized indices don't make much sense ... but hey! you can do it, if you want to (dunno why).\r\n",
    "\r\n",
    "Another way of creating Data Frames is to use dictonaries instead of lists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.DataFrame([{\"Last Name\" : \"Bond\", \"Name\" : \"James\"}, {\"Last Name\" : \"Stark\", \"Name\" : \"Tony\"}, {\"Last Name\" : \"Anderson\", \"Name\" : \"Thomas\"},{\"Last Name\" : \"Holmes\", \"Name\" : \"Sherlock\"},{\"Name\" : \"Neo\"}])\r\n",
    "print(type(df2))\r\n",
    "df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This also goes on to prove how idiotic it is to create data frames using these ways (manually). We mostly use externally created csv's or other files to import data for analysis and processing using pandas\r\n",
    "\r\n",
    "Also, notice the data type of df2!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"The column-wise mean of the data in first data frame\")\r\n",
    "print(df1.mean())\r\n",
    "print(\"\\nAnd the mean of entire data set is\")\r\n",
    "print(df1.mean().mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And this an (very, very, very leyman level) exampleof data analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Series"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(df1.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well this is a pandas' **Series** object. Series have more or less the same methonds that we can apply on a data frame object.\r\n",
    "\r\n",
    "Data Frames are made up of series. You'll get a gist once you see the below code and its result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Data type of the columns of a Data Frame is\",type(df1.Column1))\r\n",
    "print(\"Thus we can find their mean or do other (I don't know what) stuff on them.\\nLike the mean of Column1 is\", df1.Column1.mean(),\"\\nAnd the largest element of Column1 is\", df1.Column1.max())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with files/data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CSV files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Working with **csv's** and other file formats"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.read_csv(\"Dataset\\supermarkets.csv\")\r\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wasn't that easy and simple!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### json files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.read_json(\"Dataset\\supermarkets.json\")\r\n",
    "df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exel files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading **Exel** files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df3 = pd.read_excel(\"Dataset\\supermarkets.xlsx\", sheet_name = 0)\r\n",
    "df3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plain files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From **Plain txt** files (seperated by commas) we use *pandas.read_csv()* \r\n",
    "\r\n",
    "This is not technically a csv (comma seperated) but rather a *character seperated* and in case of comma we don't need to pass anyy **seperator** arguement but for other separators we need to pass the character that is used as a separator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df4 = pd.read_csv(\"Dataset\\supermarkets-commas.txt\")\r\n",
    "df4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we can check what happens if we use something other than comma as a separator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df4 = pd.read_csv(\"Dataset\\supermarkets-semi-colons.txt\")\r\n",
    "df4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, that came out crappy. I didn't recognize the separator.\r\n",
    "\r\n",
    "So what do we do"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df5 = pd.read_csv(\"Dataset\\supermarkets-semi-colons.txt\", sep = ';')\r\n",
    "df5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It worked!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Headers "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting table **Header Row**\r\n",
    "\r\n",
    "When data is imported in the code, the first row of the data is treated as the header row by default and so if data is missing header row it will create something like this"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df6 = pd.read_csv(\"Dataset\\supermarkets_noH.csv\")\r\n",
    "print(df6.sum())\r\n",
    "df6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, what the heck\r\n",
    "\r\n",
    "So we need to explicitly tell the code that the data lacks a header row by setting the **header** parameter as ***header = None***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df7 = pd.read_csv(\"Dataset\\supermarkets_noH.csv\", header = None)\r\n",
    "print(df7.sum(axis = 1))    #row-wise\r\n",
    "print(df7.sum(axis = 0))    #column-wise\r\n",
    "df7"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this tells the code that we don't havfe a header so it will give a generic header which will be made up of number indices.\r\n",
    "\r\n",
    "We can initialize the column and row names as we did earlier or like this"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naming columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df7.columns = [\"ID\", \"Address\", \"City\", \"ZIP\", \"Country\", \"Name\", \"Employees\"]\r\n",
    "df7"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may want to make a given attribute of the data as the column index, like *ID* in this above data frame.\r\n",
    "\r\n",
    "We can do this using the ***dataframe.set_index(\"name\")*** method which *returns a **new*** dataframe "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df7.set_index(\"ID\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the initial df7 is still unchanged => We need to save the output of the set_index function in some var or we need to se the **inplace** parameter of the set_index function to **True**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df8 = df7.set_index(\"ID\")\r\n",
    "df7"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **But a word of caution**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df8.set_index(\"Address\", inplace= True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now when we change the index (here we set it to Address), the new index will be set but the old index is not reassigned as a column rather is dropped or deleted"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we can do to tackle it is set **drop** parameter to ***False*** so that the attribute we set as index will also remain as a column so that later on if we change the index we'll still have the data that was earlier used for indexing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df8.set_index(\"Name\", inplace= True, drop= False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "See that Name has become an index but it will also remain as a column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indexing and extracting data\r\n",
    "\r\n",
    "We'll use df7 data frame to work upon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df7"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = df7.set_index(\"Address\", drop=True)\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use *label based* or *position based* indexing\r\n",
    "\r\n",
    "**Lable based indexing** is when we use the row and column indices for addressing them"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.loc[\"735 Dolores St\" : \"3995 23rd St\", \"Country\" : \"Name\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And specific elements using"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.loc[\"332 Hill St\", \"Country\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And for all the columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(data.loc[:, \"Country\"])   #And the list function of python will convert it in a list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Position based indexing** obviously the much better (real) way"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[1:3, 1:3]   #and as usual it is upper-bound exclusive"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[:, 1:3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[3, 1:3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also get rid of columns and rows of data frames (though these operations are not inplace, ie, they will create a new instance rather than modifying the object on which they were invoked)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deleting elements (dataframe.drop())"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **1** in the arguement of ***dataframe.drop()*** function tells that we want to delete the column and 0 implies we intend to delete rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.drop(\"City\", 1) # "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**0** removes rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.drop(\"332 Hill St\", 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To drop columns or rows based on indexing, we do a trick\r\n",
    "\r\n",
    "Example for rows is below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.drop(data.index[0:3], 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly for columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.drop(data.columns[0:3], 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***dataframe.index*** returns a list of names of all the index columns and ***dataframe.columns*** returns a series of names of all the names of columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(data.index,\"\\n\")\r\n",
    "print(data.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updating and adding **Series**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding Columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "!When adding new column of data to a data frame, we need to declare the column name and then assign a list of data values corresponding to each row (=>number of values in the list should be equal to the number of rows in the data frame)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"Continent\"] = [\"North America\"] * data.shape[0]    #Or just multiply by len(data.index)\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modifying a column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"Continent\"] = data[\"Country\"] + \", \" + \"North America\"\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding a row\r\n",
    "\r\n",
    "This ain't veryy easy as there isn't a simple way of passing a row to a data frame. So one way to doing this is to **transpose** the data frame and then adding a column like we did previously and then transpose it back"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_temp = data.T\r\n",
    "data_temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_temp[\"My Address\"] = [7, \"My City\", \"Myy ZIP code\", \"My Country\", \"My Name\", 67, \"My Continent\"]\r\n",
    "data = data_temp.T\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aaaaand we successfully added a row to our data frame\r\n",
    "\r\n",
    "And to update the data of a row we can do similar thing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modifying rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = data.T\r\n",
    "data[\"My Address\"] = [7, \"Varanasi\", \"VNS 221003\", \"India\", \"Hattori\", \"1\", \"Asia\"]\r\n",
    "data = data.T\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example of Data Analysis (Very very basic)\r\n",
    "\r\n",
    "Some tid bids:  The process of converting addresses into latitudes and longitudes is called **Geocoding** and the process of converting latitude and longitude info of a place into address is called **Reverse geocoding**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from geopy.geocoders import ArcGIS\r\n",
    "nom = ArcGIS()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = nom.geocode(\"Nav Sadhana Kendra, Varanasi, 221003, Uttar Pradesh\")\r\n",
    "print(type(n))\r\n",
    "n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oh WoW! it works\r\n",
    "\r\n",
    "Don't worry about the 0.0 in the end. It's just a response from the geocoder and doesn't mean much. Sometimes it may give a None object in case the location is incorrect (oooorr... its top secret).\r\n",
    "\r\n",
    "Also also, isn't the type of location is interesting, it is an especial object, location object of geopy. Let's extract the latitude and longitude data from it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(n.latitude, n.longitude)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets convert an entire dataframe of addresses into latitudes and longitudes and then add two columns of the extracted information to the data frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"Dataset\\supermarkets.csv\")\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well the geocoder expects from us a string as an input usually of form \"road, city, zip code, country\". Basically an address string and not a dataframe. So, lets modify the address column to meet our requirements and then pass the data to the geocoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"Address\"] = data[\"Address\"] + \", \" + data[\"City\"] + \", \" + data[\"State\"] + \", \" + data[\"Country\"]\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The **dataframe.apply** function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might be thinking of iterating, but with pandas you don't need to do that as pandas has some functions that allows you to apply a method to all the rows of a dataframe. To do that we need to create a new column/series and then assign it the output of data[\"column to apply on\"].**apply(function name)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"Coordinates\"] = data[\"Address\"].apply(nom.geocode)\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the coordinates column of the dataframe contains the location objects corresponding to the addresses \r\n",
    "\r\n",
    "Now to add two more columns to save the latitude and the longitudes of the data points separately we'll use **apply** method in conj7uction with a lambda expression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"Latitude\"] = data[\"Coordinates\"].apply(lambda x : x.latitude if x != None else None)  # added a conditional just to be sure that the scrip doesn't crash in case someone played a trick with us or made an honest mistake \r\n",
    "data[\"Longitude\"] = data[\"Coordinates\"].apply(lambda x : x.longitude if x != None else None)\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numerical and Scientific Computing with Numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is Numpy?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "NumPy is the fundamental package for scientific computing in Python. NumPy arrays facilitate advanced mathematical and other types of operations on large numbers of data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The numpy object: *ndarray*\r\n",
    "At the core of the NumPy package, is the **ndarray** object. This encapsulates n-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance. \r\n",
    "\r\n",
    "**Axis or the dimensions are referred to as 0 or 1**\r\n",
    "- 0 -> horizontal axis\r\n",
    "- 1 -> vertical axis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ndarray of single dimension: **numpy.arrange**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = np.arange(27)\r\n",
    "n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This isn't exactly a multi-dimensional array as it only has a single dimension but it is a **numpy.ndarray** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(n)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a real multidimensional array by **reshape**ing existing $1$-dim array"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n.reshape(3, 9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n.reshape(3, 3, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating numpy arrays using existing python sequences: **numpy.asarray()**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lis = [[1, 2, 3, 4, 5], [11, 22, 33, 44, 55], [111, 222, 333, 444, 555]]\r\n",
    "m = np.asarray(list)\r\n",
    "m"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting images to Numpy Arrays\r\n",
    "We'll use the **OpenCV** package, also referred to as **cv2**, for doing this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the **cv2.imread** function expects two args for loading an image.\r\n",
    "- The image itself(the path to the image to be precise)\r\n",
    "- And *int* indicating whether it is to be loaded as a **grayscales** $(0)$ or as **BGR** $(1)$ (BGR in the exact same sequence, first layer has blue, 2nd green and 3rd red)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_grey = cv2.imread(\"Dataset\\smallgray.png\", 0)\r\n",
    "img_grey"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the grayscale intensities in the 2-D array. The white (most intense) are 255 and 0 -> Black"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_bgr = cv2.imread(\"Dataset\\smallgray.png\", 1)\r\n",
    "img_bgr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the 3-D array of \r\n",
    "- Blue Layer\r\n",
    "- Green Layer\r\n",
    "- Red Layer\r\n",
    "\r\n",
    "**!!!** One thing to remember is that the **layers are transposed**, ie, the rows are vertical and the columns are horizontal, and thats why each layer is $(5$ X $3)$ instead of $(3$ x $5)$ as it should've been."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conveting Numpy arrays to images: **cv2.imwrite**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cv2.imwrite(\"new_smallgray.png\", img_grey)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Addressing elements in ndarray"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Indexing and slicing Numpy Arrays\r\n",
    "\r\n",
    "Indexing and slicing are basically the same as we do with python lists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = img_grey\r\n",
    "n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n[0:2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n[0:2, 2:4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n[2:, 2:4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets check out the shape"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iteration \r\n",
    "There are two ways to iterate over a ndarray\r\n",
    "- Row-wise or column-wise\r\n",
    "- Element-wise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Row-wise**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in n:\r\n",
    "    print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Column-wise**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in n.T:\r\n",
    "    print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **flat**tening a ndarray: n-D to 1-D\r\n",
    "We can also convert a ndarray into a single dimensional array which can be used to iterate over the array **element-wise**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in n.flat:\r\n",
    "    print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stacking and Splitting Numpy Arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stacking/Concatenating\r\n",
    "There are (obviously) two ways of stacking ndarrays\r\n",
    "- **hstack** will stack the 2 or more arrays horizontally\r\n",
    "- **vstack** will stack them vertically\r\n",
    "\r\n",
    "**!!!** Though we're concatenating multiple ndarrays these stack methods only take one argument. It takes a **tuple** of all the ndarrays to be concatenated.\r\n",
    "\r\n",
    "Also, the ndarrays to be concatenated must have **same dimensions**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_hcat = np.hstack((n, n))\r\n",
    "n_hcat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_vcat = np.vstack((n, n, n))\r\n",
    "n_vcat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting\r\n",
    "This too can be done in two ways, horizontally (**hsplit**) and vertically (**vsplit**).\r\n",
    "\r\n",
    "It takes a ndarray (the one to be split) and an integer $k$ and then divides the ndarray in $k$ equal ndarrays, horizontally or vertically (as per specified)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_hsplit = np.hsplit(n, 5)\r\n",
    "n_hsplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_vsplit = np.vsplit(n_vcat, 3)\r\n",
    "n_vsplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, notice the type of data these split methods are returning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(n_vsplit)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So **split** functions return a **python list of numpy arrays**, this implies that we can access each ndarray as we do with any python list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_vsplit[2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparation for the Finals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## Folium \r\n",
    "is a Python library used for visualizing geospatial data.\r\n",
    "\r\n",
    " Folium is a Python wrapper for Leaflet. js which is a leading open-source JavaScript library for plotting interactive maps. Thus whatever we write in python will automatically be converted into js, html and css code (as they are where the **Web Map** will run)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import folium"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Map Objects\r\n",
    "Everything in *folium* spins around its data-structure the **Map Object**. So to start working, the first thing we need to do is to create a map object using\r\n",
    "#### folium.Map\r\n",
    "Map is the class that creates the map object "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map = folium.Map(location= [38.58, -99.09])\r\n",
    "map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a map object, but we haven't converted it into a usable format\r\n",
    "## Converting map object to html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map.save(\"Dataset\\OutputMap1.html\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also play around with the zoom and webmap tiles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map2 = folium.Map(location= [38.58, -99.09], zoom_start= 6, tiles = \"Stamen Terrain\")\r\n",
    "map2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map2.save(\"Dataset\\OutputMap2.html\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding point markers and layerss to a map\r\n",
    "### **Marker**s\r\n",
    "Before saving the map objects we can add *elements* (objects) to it.\r\n",
    "\r\n",
    "**map.add_child** function  adds various elements (called children) to the map \r\n",
    "\r\n",
    "**Marker**s allows us to add pop-ups and requires us to providde it a location tuple"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map2.add_child(folium.Marker(location= [38.2, -99.1], popup= \"I'm a pop-up\", icon = folium.Icon(color= \"green\")))\r\n",
    "map2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But this isn't the best way of doing this.\r\n",
    "\r\n",
    "Rather we'd create a **folium.FeatureGroup** and add all the elements in that group and then just add the feature group to the map. This allows us to add elements more efficiently and in an organized manner. And also keeps the code itself more organized bla bla bla\r\n",
    "\r\n",
    "**Feature Groups** also allow for creating layers that can be turned on and off as per requirement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map3 = folium.Map(location= [38.58, -99.09], zoom_start= 6, tiles = \"Stamen Terrain\")\r\n",
    "feat_grp = folium.FeatureGroup(name = \"Markes\")\r\n",
    "feat_grp.add_child(folium.Marker(location= [38.2, -99.1], popup= \"Hi!, I'm a red popup\", icon= folium.Icon(color= \"red\")))\r\n",
    "feat_grp.add_child(folium.Marker(location= [39.2, -99.1], popup= \"Hi!, I'm a yellow popup\", icon= folium.Icon(color= \"blue\")))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map3.add_child(feat_grp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of this implies that for **adding multiple elements** all we need to do is to add all the elements in a common *Feature Group* like we do with any other collection by iterating over all the elements and then add that Feature Group to the *Map Object*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"Dataset\\Volcanoes.txt\", sep= \",\")   #No need for the sep arg, I added it just because\r\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now that we have our test data loaded we can see what all it contains and come to the conclusion that we only need the *LAT* and the *LON* attributes along with the *names* (maybe)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Map_multi_mark = folium.Map([42.081797, -103.696771], zoom_start= 4, tiles = \"Stamen Terrain\")\r\n",
    "Map_multi_mark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lat = list(data[\"LAT\"])\r\n",
    "lon = list()(data[\"LON\"])\r\n",
    "name = list(data[\"NAME\"])\r\n",
    "ele = list(data[\"ELEV\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have the latitude, longitude and the names in native python lists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def give_markers_grp():\r\n",
    "    Feat_grp = folium.FeatureGroup(name= \"Volcanoes Markers\")\r\n",
    "    for la, lo, na in zip(lat, lon, name):\r\n",
    "        Feat_grp.add_child(folium.Marker(location= [la,lo], popup= na, icon= folium.Icon(color= \"red\")))\r\n",
    "    return Feat_grp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Map_multi_mark.add_child(give_markers_grp())\r\n",
    "Map_multi_mark.save(\"Dataset\\OutputVolcanoes.html\")\r\n",
    "Map_multi_mark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding HTML on Popups\r\n",
    "Note that if you want to have stylized text (bold, different fonts, etc) in the popup window you can use HTML. Here's an example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"Dataset\\Volcanoes.txt\")\r\n",
    "lat = list(data[\"LAT\"])\r\n",
    "lon = list(data[\"LON\"])\r\n",
    "elev = list(data[\"ELEV\"])\r\n",
    " \r\n",
    "html = \"\"\"<h4>Volcano information:</h4>\r\n",
    "Height: %s m\r\n",
    "\"\"\"\r\n",
    " \r\n",
    "map_ex = folium.Map(location=[38.58, -99.09], zoom_start=5, tiles = \"Stamen Terrain\")\r\n",
    "fg = folium.FeatureGroup(name = \"My Map\")\r\n",
    " \r\n",
    "for lt, ln, el in zip(lat, lon, elev):\r\n",
    "    iframe = folium.IFrame(html=html % str(el), width=200, height=100)\r\n",
    "    fg.add_child(folium.Marker(location=[lt, ln], popup=folium.Popup(iframe), icon = folium.Icon(color = \"green\")))\r\n",
    "\r\n",
    "map_ex.add_child(fg)\r\n",
    "map_ex.save(\"Dataset\\OutputMap_html_popup_simple.html\")\r\n",
    "map_ex"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can even put **links** in the popup window. For example, the code below will produce a popup window with the name of the volcano as a link which does a Google search for that particular volcano when clicked:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"Dataset\\Volcanoes.txt\")\r\n",
    "lat = list(data[\"LAT\"])\r\n",
    "lon = list(data[\"LON\"])\r\n",
    "elev = list(data[\"ELEV\"])\r\n",
    "name = list(data[\"NAME\"])\r\n",
    " \r\n",
    "html = \"\"\"\r\n",
    "Volcano name:<br>\r\n",
    "<a href=\"https://www.google.com/search?q=%%22%s%%22\" target=\"_blank\">%s</a><br>\r\n",
    "Height: %s m\r\n",
    "\"\"\"\r\n",
    " \r\n",
    "map = folium.Map(location=[38.58, -99.09], zoom_start=5, tiles = \"Stamen Terrain\")\r\n",
    "fg = folium.FeatureGroup(name = \"My Map\")\r\n",
    " \r\n",
    "for lt, ln, el, name in zip(lat, lon, elev, name):\r\n",
    "    iframe = folium.IFrame(html=html % (name, name, el), width=200, height=100)\r\n",
    "    fg.add_child(folium.Marker(location=[lt, ln], popup=folium.Popup(iframe), icon = folium.Icon(color = \"green\")))\r\n",
    " \r\n",
    "map.add_child(fg)\r\n",
    "map.save(\"Dataset\\OutputMap_html_popup_advanced.html\")\r\n",
    "map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just for the sake of it, lets generate color based on elevation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def color_elev(e):\r\n",
    "    if e < 1000:\r\n",
    "        return 'green'\r\n",
    "    elif 1000 <= e < 3000:\r\n",
    "        return 'orange'\r\n",
    "    else:\r\n",
    "        return 'red'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets get markers looking all nice and good"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_cirly = folium.Map(location=[38.58, -99.09], zoom_start=5, tiles = \"Stamen Terrain\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def give_markers_grp_2():\r\n",
    "    Feat_grp = folium.FeatureGroup(name= \"Volcanoes Markers\")\r\n",
    "    for la, lo, na, e in zip(lat, lon, name, ele):\r\n",
    "        Feat_grp.add_child(folium.CircleMarker(location= [la,lo], radius= 6, popup= na, fill_color = color_elev(e), color = 'grey', fill_opacity = 0.7))\r\n",
    "    return Feat_grp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_cirly.add_child(give_markers_grp_2())\r\n",
    "map_cirly"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Map Layers\r\n",
    "Currently we have two layers in our *map object* the base map (served to us by OpenStreetMap) and the markers layer. In GIS there are several types of layers, like the point layer (that we have), polygon layer (that we'll add) and line layer (that we won't add), each with their own use-cases.\r\n",
    "\r\n",
    "To add **Polygon layer** via folium we use **folium.GeoJson()** that creates a GeoJson object (GeoJson is a special case of json), which we'll pass to the map as an element as we've done with earlier layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_grp = give_markers_grp_2()\r\n",
    "feature_grp.add_child(folium.GeoJson(data = open(\"Dataset\\world.json\", 'r', encoding= 'utf-8-sig').read()))\r\n",
    "\r\n",
    "map_poly = folium.Map(location= [42.081797, -103.696771], zoom_start= 5, tiles = \"Stamen Terrain\")\r\n",
    "map_poly.add_child(feature_grp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those polygons on the base map demarcating the countries is the third layer that we added.\r\n",
    "\r\n",
    "Let's now stylize the polygon layer based on the population of the countries *(POP2005)* to obtain something similar to a *population layer*\r\n",
    "\r\n",
    "For this we need to pass another arguement to the GeoJason"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_grp = give_markers_grp_2()\r\n",
    "feature_grp.add_child(folium.GeoJson(data = open(\"Dataset\\world.json\", 'r', encoding= 'utf-8-sig').read(),\r\n",
    "style_function = lambda x : {'fillColor' : 'yellow'}))\r\n",
    "\r\n",
    "map_im_out_of_names = folium.Map(location= [42.081797, -103.696771], zoom_start= 5, tiles = \"Stamen Terrain\")\r\n",
    "map_im_out_of_names.add_child(feature_grp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_grp = give_markers_grp_2()\r\n",
    "feature_grp.add_child(folium.GeoJson(data = open(\"Dataset\\world.json\", 'r', encoding= 'utf-8-sig').read(),\r\n",
    "style_function = lambda x : {'fillColor' : 'green' if x['properties']['POP2005'] < 10000000 \r\n",
    "else 'orange' if x['properties']['POP2005'] < 20000000 else 'red'}))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_im_out_of_names.add_child(feature_grp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding layer control panel \r\n",
    "The key thing here is the **folium.LayerControl** object that we must add as an element to our map object.\r\n",
    "\r\n",
    "**!!!** One very important thing is that the *LayerControl* looks for the *feature groups* added to the *map object* so it must be added to the *map object* after we've already added all the layers (that are to be controled) to our map."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_final__promis = folium.Map(location= [42.081797, -103.696771], zoom_control= 6, tiles = \"Stamen Terrain\")\r\n",
    "feature_grp_final1 = give_markers_grp_2()\r\n",
    "feature_grp_final12 = folium.FeatureGroup(name = \"Population Map\")\r\n",
    "feature_grp_final12.add_child(folium.GeoJson(data = open(\"Dataset\\world.json\", 'r', encoding= 'utf-8-sig').read(),\r\n",
    "style_function = lambda x : {'fillColor' : 'green' if x['properties']['POP2005'] < 10000000 \r\n",
    "else 'orange' if x['properties']['POP2005'] < 20000000 else 'red'}))\r\n",
    "map_final__promis.add_child(feature_grp_final1)\r\n",
    "map_final__promis.add_child(feature_grp_final12)\r\n",
    "map_final__promis.add_child(folium.LayerControl())\r\n",
    "map_final__promis.save(\"Dataset\\OutputFinal.html\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9da5a1eb768521143534bf09b996f0d525d45d2d4b488be6c4304e80c1815fea"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}